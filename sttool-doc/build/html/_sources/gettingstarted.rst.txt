Getting Started with ST-Tool
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
This chapter introduces how to use ST-Tool to run some applications. When working in a distributed system, the whole package has to be stored on the master server while the data should be stored in HDFS. All the operations are done on the master server unless otherwise specified.

Preparation
-----------

Modify Config.scala
>>>>>>>>>>>>>>>>>>>

Firstly, please modify ``ST-TOOL/src/main/scala/Config.scala`` accordingly. ``Config.scala`` contains information for two environments, namely *local* and *distributed*.
Each configuration set should have the following fields ::
    {
        "master" -> "local[*]", // the spark master url 
        "numPartitions" -> "16", // number of partitions 
        "hzData" -> "datasets/traj_10000_converted.json", // path to the dataset, can be a directory in HDFS
        "resPath" -> "out/", // output path, can be a directyory in HDFS. Make sure that the path exists.
        "tPartition" -> "4", // number of partitions in temporal dimension
        "samplingRate" -> "0.5", // sampling rate for partitioning boundary selection and etc.
    }



Build the Application Project
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

Please build the project by typing 

.. code-block:: bashscript    

     sbt package

at the root directory of ``ST-TOOL/``.

sbt uses ``ST-TOOL/build.sbt`` to build the project. You may modify it accordingly. The compiled ``.jar`` file is located at 
``ST-TOOL/target/scala-2.12/classes/st-tool-app_2.12-0.1.jar``

The examples below are all based on **PointAnalysisApp** and the code is located at ``ST-TOOL/src/main/scala/PointAnalysisApp``. 

Submit Example Job Using spark-submit
--------------------------------------

An example bashscript file for submitting the spark task can be found at ``ST-TOOL/scripts/example-spark-submit.sh``. 

It is basically a common ``spark-submit`` command and you may adjust the parameters according to the acutal distributed environment.

The explamentation of the parameters can be found at `Spark Documentation <https://spark.apache.org/docs/latest/submitting-applications.html>`_.

Example output of **PointAnalysisApp** ::

    ... before selection 7486 trajectories
    === Analysing Results:
     ... Total number of trajectories: 836
    ... Total number of points: 26900
     ... Top 3 most frequent:
    ....  (c1a6b0b2-20f9-34bd-ba6e-7127a9bc916b,996)
    ....  (808a52d1-9ac9-30a5-8506-85c5e32d8a20,493)
    ....  (74d8398a-dac5-335f-a21a-e28dc5af8e29,368)


Submit Example Job Using Spark REST API
----------------------------------------

*TO BE FILLED*

Write Your First Application with ST-Tool
----------------------------------------

``ST-TOOL/src/main/scala/PointAnalysisApp`` can be used as a template for application writing. 

Following is a step-by-step example of writing a CompanionExtractionApp (can be found at ``ST-TOOL/src/main/scala/CompanionApp``)

- Set up Spark environment 

.. code-block:: scala    

    // common practice, only the appName needs to be changed
    val spark = SparkSession
        .builder()
        .appName("PointAnalysisApp")
        .master(Config.get("master"))
        .getOrCreate()
    val sc = spark.sparkContext
    sc.setLogLevel("ERROR")


- Parse input arguments 

.. code-block:: scala    

    // application specific;
    // the arguments are read from Config.scala 
    // or input by the user during spark-submit
    val trajectoryFile = Config.get("hzData") // path to the trajectory file
    val numPartitions = Config.get("numPartitions").toInt // number of partitions
    val sQuery = Rectangle(args(0).split(",").map(_.toDouble)) // spatial range of selection
    val tQuery = parseTemporalRange(args(1)) //temporal range of selection, should be one day with some margin (e.g. half an hour)
    val queryThreshold = args(2).split(",").map(_.toDouble) // (spatial threshold, temporal threshold) for defining companion (unit: meter and second)
    val sThreshold = queryThreshold.head
    val tThreshold = queryThreshold.last

    /**
    * example input arguments: "118.35,29.183,120.5,30.55" "2020-07-31 23:30:00,2020-08-02 00:30:00" "500,600"
    */
    

- Initialize operators

.. code-block:: scala    

    // the operator set consists of three components;
    // default selector is normally used;
    // st-tool provides most common convertors;
    // the extractor is customized and in customExtractor package
    val operator = new CustomOperatorSet(
        DefaultSelector(numPartitions),
        new Traj2PointConverter(Some(sQuery), Some(tQuery)),
        new CompanionExtractor)

- Read input data

.. code-block:: scala    

        // use st-tool build-in function to read the trajectory data;
        // if the data is with different format, a customized function can be applied;
        // make sure that the result is RDD[Point] or RDD[Trajectory]
        val trajRDD = ReadTrajJson(trajectoryFile, numPartitions)

        


- The three steps of calculation
  
.. code-block:: scala    

        // normally no need of changes

        /** step 1: Selection */
        val rdd1 = operator.selector.query(trajRDD, sQuery, tQuery)
        
        /** step 2: Conversion */
        val rdd2 = operator.converter.convert(rdd1)

        // optional
        rdd2.persist(StorageLevel.MEMORY_AND_DISK_SER)
        println(s"--- Total points: ${rdd2.count}")
        println(s"... Total ids: ${rdd2.map(_.attributes("tripID")).distinct.count}")
        
        /** step 3: Extraction */
        val companionPairsRDD = operator.extractor.optimizedExtract(sThreshold,
         tThreshold, Config.get("tPartition").toInt)(rdd2)
         
        // optional
        companionPairsRDD.persist(StorageLevel.MEMORY_AND_DISK_SER)
        rdd2.unpersist()
        println("=== Start companion analysis: ")
        
        // print 5 examples
        println("=== 5 examples: ")
        companionPairsRDD.take(5).foreach { case (q, c) => println(s"  ... $q: $c") }
        
- Save results to HDFS as Json files

.. code-block:: scala    

        val schema = StructType(
         Seq(
           StructField("ID", StringType, nullable = false),
           StructField("companion", MapType(LongType, StringType, valueContainsNull = false), nullable = false)
         )

        val resDF = spark.createDataFrame(companionPairsRDD.map(x => Row(x._1, x._2)), schema)
        resDF.write.json(Config.get("resPath") + nextDay(tQuery._1))
        

